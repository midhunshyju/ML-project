# -*- coding: utf-8 -*-
"""Loan Approval Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NTDQnB0cIgWIzRqFcXTVdEpJnRbjz0IC

# ***Importing Libraries***
"""

import pandas as pd
import numpy as np
import pickle
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import MinMaxScaler,StandardScaler
from sklearn.feature_selection import chi2
from sklearn.model_selection import train_test_split,GridSearchCV, RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report,confusion_matrix,roc_curve,auc,ConfusionMatrixDisplay
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import roc_curve, auc
from scipy.stats.mstats import winsorize
import warnings
warnings.filterwarnings("ignore")

"""# ***Reading Data set***"""

df=pd.read_csv('/content/loan_data.csv')
df

warnings.filterwarnings("ignore")

df.info()

df.shape

"""# ***Checking missing value***"""

df.isna().sum()

"""# ***Checking data type***"""

df.dtypes

df_numeric = df.select_dtypes(include=['number'])
plt.figure(figsize=(18, 6))
sns.boxplot(data=df_numeric)
plt.title('Before Winsorization')
plt.show()

df_winsorized = df_numeric.copy()
for col in df_numeric.columns:
    df_winsorized[col] = winsorize(df_numeric[col], limits=[0.05, 0.05])
plt.figure(figsize=(18, 6))
sns.boxplot(data=df_winsorized)
plt.title('After Winsorization')
plt.show()

categorical_vars = ['person_gender', 'person_education', 'person_home_ownership', 'loan_intent', 'previous_loan_defaults_on_file']
for var in categorical_vars:
    sns.catplot(
        data=df,
        x=var,
        col='loan_status',
        kind='count',
        col_wrap=4,
        height=4,
        aspect=1.2,
        palette='coolwarm'
    )
plt.show()

"""Note: For credit approval, the customer's history must be free of any filed defaults.

# **Apply Label encoder**
"""

categorical_columns= ['person_gender', 'person_education', 'person_home_ownership', 'loan_intent', 'previous_loan_defaults_on_file']
encoder=LabelEncoder()
for col in categorical_columns:
    df[col] = encoder.fit_transform(df[col])

plt.figure(figsize=(10, 8))
correlation_matrix = df.corr()
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap='coolwarm')
plt.title('Correlation Heatmap')
plt.show()

"""1.   Person's age and employment experience show a strong positive correlation
2.   List item

# ***Separating input and output***
"""

X=df.iloc[:,:-1]
y=df.iloc[:,-1]

"""# ***Preprocessing***"""

scaler=MinMaxScaler()
X_scaled=scaler.fit_transform(X)
X_scaled

X_train,X_test,y_train,y_test=train_test_split(X_scaled,y,test_size=0.3,random_state=42)

"""# ***Model building***"""

knn=KNeighborsClassifier(n_neighbors=5)
svc=SVC(kernel='poly')
nb=GaussianNB()
adb = AdaBoostClassifier(n_estimators=100, learning_rate=0.1)
dt=DecisionTreeClassifier(criterion='entropy')
rf=RandomForestClassifier(n_estimators=100, random_state=42)
gb=GradientBoostingClassifier(n_estimators=100)
xgb = XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=6)

models=[knn,svc,nb,adb,dt,rf,gb,xgb]
acc=[]
for model in models:
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    print(f'\nModel: {model.__class__.__name__}')
    print('Classification Report:\n', classification_report(y_test, y_pred))
    print(ConfusionMatrixDisplay.from_predictions(y_test, y_pred))
    acc.append(accuracy_score(y_test, y_pred))

models_names=['knn','svc','nb','adb','dt','rf','gb','xgb']
plt.figure(figsize=(10, 6))
plt.bar(models_names,acc,color='skyblue')
plt.xlabel('Models')
plt.ylabel('Accuracy (%)')
plt.title('Model Accuracy Comparison')
plt.ylim(0, 1)

y.value_counts()

"""# ***Over sampling***"""

from imblearn.over_sampling import SMOTE
os=SMOTE(random_state=1)
X_os,y_os=os.fit_resample(X,y)
y_os.value_counts()

X_os_scaled=scaler.fit_transform(X_os)

X_ostrain,X_ostest,y_ostrain,y_ostest=train_test_split(X_os_scaled,y_os,test_size=0.3,random_state=42)
X_ostrain.shape,y_ostrain.shape

for model in models:
    model.fit(X_ostrain, y_ostrain)
    y_pred2 = model.predict(X_ostest)
    print(f'\nModel: {model.__class__.__name__}')
    print('Classification Report:\n', classification_report(y_ostest, y_pred2))

X_os.columns

"""# ***Feature Selection***"""

features =['person_age', 'person_gender', 'person_education', 'person_income',
       'person_emp_exp', 'person_home_ownership', 'loan_amnt', 'loan_intent',
       'loan_int_rate', 'loan_percent_income', 'cb_person_cred_hist_length',
       'credit_score', 'previous_loan_defaults_on_file']
ch_values, p_values = chi2(X_os_scaled, y_os)
chi2_result = pd.DataFrame({'Features': features, 'Chi2 Score': ch_values, 'p-value': p_values})
print(chi2_result)

best_features = chi2_result[chi2_result['p-value'] > 0.05]
print(best_features)

X_os.drop(['person_age','cb_person_cred_hist_length','credit_score'] ,axis=1,inplace=True)

scaler=MinMaxScaler()
X_new_scaler=scaler.fit_transform(X_os)

X_new_train,X_new_test,y_new_train,y_new_test=train_test_split(X_new_scaler,y_os,test_size=0.3,random_state=42)
X_new_train.shape,y_new_train.shape

acc1=[]

for model in models:
    model.fit(X_new_train, y_new_train)
    y_pred3 = model.predict(X_new_test)
    print(f'\nModel: {model.__class__.__name__}')
    print('Classification Report:\n', classification_report(y_new_test, y_pred3))
    acc1.append(accuracy_score(y_new_test,y_pred3))

models_names=['knn','svc','nb','adb','dt','rf','gb','xgb']
plt.figure(figsize=(10, 6))
plt.bar(models_names,acc1,color='r')
plt.xlabel('Models')
plt.ylabel('Accuracy (%)')
plt.title('Model Accuracy Comparison')
plt.ylim(0, 1)

"""Inference:RandomForest classifier is the best based on performance

# ***Hyper Parameter tunning***
"""

param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['auto', 'sqrt', 'log2'],
    'bootstrap': [True, False]
}

from sklearn.model_selection import RandomizedSearchCV
clf = RandomizedSearchCV(rf, param_distributions=param_grid, cv=7, scoring='accuracy')
clf.fit(X_new_train, y_new_train)

clf.best_params_

pred=clf.predict(X_new_test)
print(classification_report(y_new_test,pred))

cases=['Imbalanced','Balanced','Feature Selection ',' HyperparameterTuning']
test=[y_test,y_ostest,y_new_test,y_new_test]
preds=[y_pred,y_pred2,y_pred3,pred]
scores=[]
for i in range (4):
  scores.append(accuracy_score(test[i],preds[i]))
plt.figure(figsize=(8,6))
plt.title('Ranomforestclassifier Performance at different stage')
plt.bar(cases,scores)
plt.show()

model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 5, 10, 20],
    'min_child_weight': [1, 3, 5],
    'subsample': [0.6, 0.8, 1.0]
}
grid_search = GridSearchCV(model, param_grid, cv=5, scoring='roc_auc')
grid_search.fit(X_train, y_train)
best_model = grid_search.best_estimator_

y_scores = best_model.predict_proba(X_test)[:, 1]
fpr, tpr, thresholds = roc_curve(y_test, y_scores)
roc_auc = auc(fpr, tpr)
plt.figure()
plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], 'k--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

"""# **Saving the model **"""

import pickle

pickle.dump(clf, open('Placement.sav', 'wb'))
pickle.dump(scaler, open('scaler.sav', 'wb'))

