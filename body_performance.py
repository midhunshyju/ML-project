# -*- coding: utf-8 -*-
"""BODY_PERFORMANCE.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RVzcABI33lXUPyN3n4lSqbyxGDkQ06Wd
"""

import pandas as pd
import numpy as np
import pickle
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import MinMaxScaler,StandardScaler
from sklearn.feature_selection import chi2
from sklearn.model_selection import train_test_split,GridSearchCV, RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report,confusion_matrix,roc_curve,auc,ConfusionMatrixDisplay
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import roc_curve, auc
from scipy.stats.mstats import winsorize
import warnings

df=pd.read_csv('/content/bodyPerformance.csv')
df

df.shape

warnings.filterwarnings("ignore")

df.isna().sum()

df.dtypes

df_numeric = df.select_dtypes(include=['number'])
plt.figure(figsize=(18, 6))
sns.boxplot(data=df_numeric)
plt.title('Before Winsorization')
plt.show()

df_winsorized = df_numeric.copy()
for col in df_numeric.columns:
    df_winsorized[col] = winsorize(df_numeric[col], limits=[0.05, 0.05])
plt.figure(figsize=(18, 6))
sns.boxplot(data=df_winsorized)
plt.title('After Winsorization')
plt.show()

plt.figure(figsize=(6,5))
plt.title('class distribution')
sns.countplot(data=df, x='class')
plt.xlabel('class')
plt.ylabel('count')
plt.show()

plt.figure(figsize=(6,5))
plt.title('class vs gripforce')
sns.boxplot(data=df, x='class', y='gripForce')
plt.xlabel('class')
plt.ylabel('count')
plt.show()

encoder=LabelEncoder()
categorical_df = df.select_dtypes(include='object')
categorical_cols=categorical_df.columns
for col in categorical_cols:
  df[col]=encoder.fit_transform(df[col])

df

X=df.iloc[:,:-1]
y=df.iloc[:,-1]

scaler=MinMaxScaler()
X_scaler=scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X_scaler,y,test_size=0.3,random_state=42)

knn=KNeighborsClassifier()
svc=SVC()
nb=GaussianNB()
adb = AdaBoostClassifier()
dt=DecisionTreeClassifier()
rf=RandomForestClassifier()
gb=GradientBoostingClassifier()
xgb = XGBClassifier()

from sklearn.metrics import accuracy_score, classification_report
acc=[]
models = {
    'KNN': knn,
    'SVC (Poly Kernel)': svc,
    'Naive Bayes': nb,
    'AdaBoost': adb,
    'Decision Tree': dt,
    'Random Forest': rf,
    'Gradient Boosting': gb,
    'XGBoost': xgb
}

for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    print(f"Model: {name}")
    print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")
    print("Classification Report:")
    print(classification_report(y_test, y_pred))
    ConfusionMatrixDisplay.from_predictions(y_test, y_pred)
    acc.append(accuracy_score(y_test, y_pred))
    print("-" * 50)

models_names=['knn','svc','nb','adb','dt','rf','gb','xgb']
plt.figure(figsize=(10, 6))
plt.bar(models_names,acc,color='skyblue')
plt.xlabel('Models')
plt.ylabel('Accuracy (%)')
plt.title('Model Accuracy Comparison')
plt.ylim(0, 1)

y.value_counts()

from imblearn.over_sampling import SMOTE
os=SMOTE(random_state=1)
X_os,y_os=os.fit_resample(X,y)
y_os.value_counts()

X_os_scaled=scaler.fit_transform(X_os)

X_ostrain,X_ostest,y_ostrain,y_ostest=train_test_split(X_os_scaled,y_os,test_size=0.3,random_state=42)
X_ostrain.shape,y_ostrain.shape

acc1=[]
models1 = {
    'KNN': knn,
    'SVC (Poly Kernel)': svc,
    'Naive Bayes': nb,
    'AdaBoost': adb,
    'Decision Tree': dt,
    'Random Forest': rf,
    'Gradient Boosting': gb,
    'XGBoost': xgb
}

for name, model in models1.items():
    model.fit(X_ostrain, y_ostrain)
    y_pred1 = model.predict(X_ostest)

    print(f"Model: {name}")
    print(f"Accuracy: {accuracy_score(y_ostest, y_pred1):.4f}")
    print("Classification Report:")
    print(classification_report(y_ostest, y_pred1))
    ConfusionMatrixDisplay.from_predictions(y_ostest, y_pred1)
    acc1.append(accuracy_score(y_ostest, y_pred1))
    print("-" * 50)

models_names=['knn','svc','nb','adb','dt','rf','gb','xgb']
plt.figure(figsize=(10, 6))
plt.bar(models_names,acc1,color='skyblue')
plt.xlabel('Models')
plt.ylabel('Accuracy (%)')
plt.title('Model Accuracy Comparison')
plt.ylim(0, 1)

numeric_df = df.select_dtypes(include='number')
corr = numeric_df.corr()

plt.figure(figsize=(10, 8))
plt.title('correlation heatmap')
sns.heatmap(corr, annot=True, cmap='coolwarm')

# X_os.columns

# features =['weight_kg', 'body fat_%',
#        'sit and bend forward_cm', 'sit-ups counts',
#        'broad jump_cm']

# X = df[features]
# y = df['class']

# scaler=MinMaxScaler()
# X_new_scaler=scaler.fit_transform(X)

# X_new_train,X_new_test,y_new_train,y_new_test=train_test_split(X_new_scaler,y,test_size=0.3,random_state=42)
# X_new_train.shape,y_new_train.shape

# acc2=[]
# models_1 = {
#     'KNN': knn,
#     'SVC (Poly Kernel)': svc,
#     'Naive Bayes': nb,
#     'AdaBoost': adb,
#     'Decision Tree': dt,
#     'Random Forest': rf,
#     'Gradient Boosting': gb,
#     'XGBoost': xgb
# }

# for name, model in models1.items():
#     model.fit(X_new_train, y_new_train)
#     y_pred2 = model.predict(X_new_test)

#     print(f"Model: {name}")
#     print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")
#     print("Classification Report:")
#     print(classification_report(y_new_test, y_pred2))
#     ConfusionMatrixDisplay.from_predictions(y_new_test, y_pred2)
#     acc2.append(accuracy_score(y_new_test, y_pred2))
#     print("-" * 50)

xgb_param_grid = {
    'learning_rate': [0.01, 0.05, 0.1, 0.2],
    'max_depth': [3, 5, 7, 9],
    'min_child_weight': [1, 3, 5],
    'subsample': [0.6, 0.8, 1.0],
    'colsample_bytree': [0.6, 0.8, 1.0],
    'gamma': [0, 0.1, 0.2],
    'n_estimators': [100, 200, 300],
    'reg_alpha': [0, 0.1, 1],
    'reg_lambda': [0.1, 1, 5]
}


xgb_random = RandomizedSearchCV(
    xgb,
    param_distributions=xgb_param_grid,
    cv=7,
    scoring='accuracy',
    n_iter=20,
    random_state=42,
    n_jobs=-1,
    verbose=2
)

# Fit the model
xgb_random.fit(X_ostrain, y_ostrain)

# Print best parameters
print("Best XGBoost Parameters:")
print(xgb_random.best_params_)

# Make predictions and evaluate
xgb_pred = xgb_random.predict(X_ostest)
print("\nXGBoost Classification Report:")
print(classification_report(y_ostest, xgb_pred))

# cases=['Imbalanced','Balanced','Feature Selection ',' HyperparameterTuning']
# test=[y_test,y_ostest,y_ostest]
# preds=[y_pred,y_pred1,xgb_pred ]
# scores=[]
# for i in range (3):
#   scores.append(accuracy_score(test[i],preds[i]))
# plt.figure(figsize=(8,6))
# plt.title('XGBClassifier Performance at different stage')
# plt.bar(cases,scores)
# plt.show()

model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')
param_grid = {
    'learning_rate': [0.01, 0.05, 0.1, 0.2],
    'max_depth': [3, 5, 7, 9],
    'min_child_weight': [1, 3, 5],
    'subsample': [0.6, 0.8, 1.0],
    'colsample_bytree': [0.6, 0.8, 1.0],
    'gamma': [0, 0.1, 0.2],
    'n_estimators': [100, 200, 300],
    'reg_alpha': [0, 0.1, 1],
    'reg_lambda': [0.1, 1, 5]
}

grid_search = GridSearchCV(model, param_grid, cv=5, scoring='roc_auc')
grid_search.fit(X_train, y_train)
best_model = grid_search.best_estimator_

y_scores = best_model.predict_proba(X_test)[:, 1]
fpr, tpr, thresholds = roc_curve(y_test, y_scores)
roc_auc = auc(fpr, tpr)
plt.figure()
plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], 'k--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

import pickle

pickle.dump(xgb_random, open('Body_performance', 'wb'))
pickle.dump(scaler, open('scaler.sav', 'wb'))